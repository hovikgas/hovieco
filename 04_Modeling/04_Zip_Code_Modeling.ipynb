{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupdf = pd.read_csv('./data/groupdf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>totalvalue</th>\n",
       "      <th>latitude_x</th>\n",
       "      <th>longitude_x</th>\n",
       "      <th>logvalue</th>\n",
       "      <th>review_count</th>\n",
       "      <th>latitude_y</th>\n",
       "      <th>longitude_y</th>\n",
       "      <th>log_reviews</th>\n",
       "      <th>price_1.0</th>\n",
       "      <th>...</th>\n",
       "      <th>type_Venues</th>\n",
       "      <th>type_Vietnamese</th>\n",
       "      <th>type_Vitaminssupplements</th>\n",
       "      <th>type_Waffles</th>\n",
       "      <th>type_Whiskeybars</th>\n",
       "      <th>type_Wine_Bars</th>\n",
       "      <th>type_Winetastingroom</th>\n",
       "      <th>type_Womenscloth</th>\n",
       "      <th>type_Wraps</th>\n",
       "      <th>type_Yoga</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90001</td>\n",
       "      <td>2.924906e+05</td>\n",
       "      <td>33.968543</td>\n",
       "      <td>-118.261693</td>\n",
       "      <td>12.564232</td>\n",
       "      <td>8588</td>\n",
       "      <td>6353.401556</td>\n",
       "      <td>-22112.494475</td>\n",
       "      <td>502.254649</td>\n",
       "      <td>135.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90002</td>\n",
       "      <td>2.870877e+05</td>\n",
       "      <td>33.946024</td>\n",
       "      <td>-118.250578</td>\n",
       "      <td>12.551515</td>\n",
       "      <td>1081</td>\n",
       "      <td>1459.928001</td>\n",
       "      <td>-5084.555034</td>\n",
       "      <td>98.170200</td>\n",
       "      <td>39.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90003</td>\n",
       "      <td>2.972847e+05</td>\n",
       "      <td>33.961248</td>\n",
       "      <td>-118.273066</td>\n",
       "      <td>12.573247</td>\n",
       "      <td>4701</td>\n",
       "      <td>4755.757869</td>\n",
       "      <td>-16558.215243</td>\n",
       "      <td>383.669881</td>\n",
       "      <td>118.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90004</td>\n",
       "      <td>1.024344e+06</td>\n",
       "      <td>34.077047</td>\n",
       "      <td>-118.313083</td>\n",
       "      <td>13.743391</td>\n",
       "      <td>69031</td>\n",
       "      <td>7939.550715</td>\n",
       "      <td>-27565.505661</td>\n",
       "      <td>1115.333722</td>\n",
       "      <td>98.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90005</td>\n",
       "      <td>1.113551e+06</td>\n",
       "      <td>34.058708</td>\n",
       "      <td>-118.319786</td>\n",
       "      <td>13.858325</td>\n",
       "      <td>106745</td>\n",
       "      <td>7901.772113</td>\n",
       "      <td>-27445.860659</td>\n",
       "      <td>1236.250662</td>\n",
       "      <td>73.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 259 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   location    totalvalue  latitude_x  longitude_x   logvalue  review_count  \\\n",
       "0     90001  2.924906e+05   33.968543  -118.261693  12.564232          8588   \n",
       "1     90002  2.870877e+05   33.946024  -118.250578  12.551515          1081   \n",
       "2     90003  2.972847e+05   33.961248  -118.273066  12.573247          4701   \n",
       "3     90004  1.024344e+06   34.077047  -118.313083  13.743391         69031   \n",
       "4     90005  1.113551e+06   34.058708  -118.319786  13.858325        106745   \n",
       "\n",
       "    latitude_y   longitude_y  log_reviews  price_1.0  ...  type_Venues  \\\n",
       "0  6353.401556 -22112.494475   502.254649      135.0  ...          0.0   \n",
       "1  1459.928001  -5084.555034    98.170200       39.0  ...          0.0   \n",
       "2  4755.757869 -16558.215243   383.669881      118.0  ...          0.0   \n",
       "3  7939.550715 -27565.505661  1115.333722       98.0  ...          0.0   \n",
       "4  7901.772113 -27445.860659  1236.250662       73.0  ...          0.0   \n",
       "\n",
       "   type_Vietnamese  type_Vitaminssupplements  type_Waffles  type_Whiskeybars  \\\n",
       "0              0.0                       0.0           0.0               0.0   \n",
       "1              0.0                       0.0           0.0               0.0   \n",
       "2              0.0                       0.0           0.0               0.0   \n",
       "3              2.0                       0.0           0.0               0.0   \n",
       "4              4.0                       0.0           0.0               0.0   \n",
       "\n",
       "   type_Wine_Bars  type_Winetastingroom  type_Womenscloth  type_Wraps  \\\n",
       "0             0.0                   0.0               0.0         0.0   \n",
       "1             0.0                   0.0               0.0         0.0   \n",
       "2             0.0                   0.0               0.0         0.0   \n",
       "3             0.0                   0.0               0.0         0.0   \n",
       "4             0.0                   0.0               0.0         0.0   \n",
       "\n",
       "   type_Yoga  \n",
       "0        0.0  \n",
       "1        0.0  \n",
       "2        0.0  \n",
       "3        0.0  \n",
       "4        0.0  \n",
       "\n",
       "[5 rows x 259 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting our primary explanatory variables for our baseline model with just the top 6 restaurant types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = groupdf[['price_1.0',\n",
    "             'price_2.0', \n",
    "            'price_3.0', \n",
    "            'price_4.0', \n",
    "            'rating_1.0', \n",
    "            'rating_2.0', \n",
    "            'rating_2.5', \n",
    "            'rating_3.0',\n",
    "            'rating_3.5', \n",
    "            'rating_4.0', \n",
    "            'rating_4.5', \n",
    "            'rating_5.0', \n",
    "           'type_Mexican',\n",
    "           'type_Coffee',\n",
    "           'type_Pizza',\n",
    "           'type_Hotdogs',\n",
    "           'type_Burgers',\n",
    "           'type_Bakeries',\n",
    "            'log_reviews']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting our outcome variable as the log-transformed home values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = groupdf.logvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiating a Linear regression model, train/test splitting, and scaling the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sc = sc.fit_transform(X_train)\n",
    "X_test_sc = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.694967392695512"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0974639276197198"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_test_sc, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it seems like our baseline linear regression model is very overfit on the training set and due to multicollinearity issues, fails to converge on the test set. Thus, going to try Lasso and Ridge, to see if we can ameliorate some of the multicollinearity issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5642698015054128"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso = Lasso(alpha=3, max_iter=50000)\n",
    "lasso.fit(X_train, y_train)\n",
    "lasso.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2705440575138798"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the score for the training set dropped from .69 to .59, but at least for the test set we have an actual score of .27. I tried various alphas, and three ended up with the highest scores. Next we'll try it with Ridge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6653061306317665"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge = Ridge(alpha=1000)\n",
    "ridge.fit(X_train, y_train)\n",
    "ridge.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2541392669246998"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite cranking up the alpha to 1000 (after that, the score did not increase any more), the ridge regression did not perform as well as Lasso, so now I'm going to try out Random Forests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9329949018504071"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3758669494527853"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like Random Forests has the best score thus far. After various hypertuning, without much gain in the scores on the test set, we decided to leave the default hyperparameters. To be able to interpret the statistical significance of our coeeficients, I am now going to try Statsmodels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sm.add_constant(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(y, X).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>logvalue</td>     <th>  R-squared:         </th> <td>   0.621</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.534</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   7.158</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 29 Apr 2019</td> <th>  Prob (F-statistic):</th> <td>7.61e-11</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>15:11:53</td>     <th>  Log-Likelihood:    </th> <td> -27.519</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   103</td>      <th>  AIC:               </th> <td>   95.04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    83</td>      <th>  BIC:               </th> <td>   147.7</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    19</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td></td>           <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>         <td>   13.5778</td> <td>    0.088</td> <td>  154.939</td> <td> 0.000</td> <td>   13.404</td> <td>   13.752</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>price_1.0</th>     <td>   -0.0264</td> <td>    0.011</td> <td>   -2.317</td> <td> 0.023</td> <td>   -0.049</td> <td>   -0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>price_2.0</th>     <td>   -0.0167</td> <td>    0.012</td> <td>   -1.407</td> <td> 0.163</td> <td>   -0.040</td> <td>    0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>price_3.0</th>     <td>   -0.0191</td> <td>    0.012</td> <td>   -1.577</td> <td> 0.119</td> <td>   -0.043</td> <td>    0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>price_4.0</th>     <td>   -0.0233</td> <td>    0.017</td> <td>   -1.401</td> <td> 0.165</td> <td>   -0.056</td> <td>    0.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rating_1.0</th>    <td>    0.0051</td> <td>    0.042</td> <td>    0.121</td> <td> 0.904</td> <td>   -0.078</td> <td>    0.089</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rating_2.0</th>    <td>    0.0184</td> <td>    0.017</td> <td>    1.114</td> <td> 0.268</td> <td>   -0.014</td> <td>    0.051</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rating_2.5</th>    <td>    0.0270</td> <td>    0.016</td> <td>    1.738</td> <td> 0.086</td> <td>   -0.004</td> <td>    0.058</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rating_3.0</th>    <td>    0.0234</td> <td>    0.013</td> <td>    1.832</td> <td> 0.071</td> <td>   -0.002</td> <td>    0.049</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rating_3.5</th>    <td>    0.0230</td> <td>    0.013</td> <td>    1.733</td> <td> 0.087</td> <td>   -0.003</td> <td>    0.049</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rating_4.0</th>    <td>    0.0184</td> <td>    0.014</td> <td>    1.341</td> <td> 0.184</td> <td>   -0.009</td> <td>    0.046</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rating_4.5</th>    <td>    0.0197</td> <td>    0.013</td> <td>    1.542</td> <td> 0.127</td> <td>   -0.006</td> <td>    0.045</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rating_5.0</th>    <td>    0.0104</td> <td>    0.016</td> <td>    0.651</td> <td> 0.517</td> <td>   -0.021</td> <td>    0.042</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>type_Mexican</th>  <td>   -0.0009</td> <td>    0.005</td> <td>   -0.177</td> <td> 0.860</td> <td>   -0.011</td> <td>    0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>type_Coffee</th>   <td>   -0.0126</td> <td>    0.009</td> <td>   -1.443</td> <td> 0.153</td> <td>   -0.030</td> <td>    0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>type_Pizza</th>    <td>    0.0050</td> <td>    0.008</td> <td>    0.625</td> <td> 0.534</td> <td>   -0.011</td> <td>    0.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>type_Hotdogs</th>  <td>   -0.0094</td> <td>    0.013</td> <td>   -0.735</td> <td> 0.464</td> <td>   -0.035</td> <td>    0.016</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>type_Burgers</th>  <td>   -0.0120</td> <td>    0.010</td> <td>   -1.238</td> <td> 0.219</td> <td>   -0.031</td> <td>    0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>type_Bakeries</th> <td>    0.0186</td> <td>    0.010</td> <td>    1.897</td> <td> 0.061</td> <td>   -0.001</td> <td>    0.038</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>log_reviews</th>   <td>    0.0002</td> <td>    0.001</td> <td>    0.262</td> <td> 0.794</td> <td>   -0.002</td> <td>    0.002</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 5.425</td> <th>  Durbin-Watson:     </th> <td>   1.401</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.066</td> <th>  Jarque-Bera (JB):  </th> <td>   4.780</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.473</td> <th>  Prob(JB):          </th> <td>  0.0917</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.468</td> <th>  Cond. No.          </th> <td>2.70e+03</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 2.7e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:               logvalue   R-squared:                       0.621\n",
       "Model:                            OLS   Adj. R-squared:                  0.534\n",
       "Method:                 Least Squares   F-statistic:                     7.158\n",
       "Date:                Mon, 29 Apr 2019   Prob (F-statistic):           7.61e-11\n",
       "Time:                        15:11:53   Log-Likelihood:                -27.519\n",
       "No. Observations:                 103   AIC:                             95.04\n",
       "Df Residuals:                      83   BIC:                             147.7\n",
       "Df Model:                          19                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=================================================================================\n",
       "                    coef    std err          t      P>|t|      [0.025      0.975]\n",
       "---------------------------------------------------------------------------------\n",
       "const            13.5778      0.088    154.939      0.000      13.404      13.752\n",
       "price_1.0        -0.0264      0.011     -2.317      0.023      -0.049      -0.004\n",
       "price_2.0        -0.0167      0.012     -1.407      0.163      -0.040       0.007\n",
       "price_3.0        -0.0191      0.012     -1.577      0.119      -0.043       0.005\n",
       "price_4.0        -0.0233      0.017     -1.401      0.165      -0.056       0.010\n",
       "rating_1.0        0.0051      0.042      0.121      0.904      -0.078       0.089\n",
       "rating_2.0        0.0184      0.017      1.114      0.268      -0.014       0.051\n",
       "rating_2.5        0.0270      0.016      1.738      0.086      -0.004       0.058\n",
       "rating_3.0        0.0234      0.013      1.832      0.071      -0.002       0.049\n",
       "rating_3.5        0.0230      0.013      1.733      0.087      -0.003       0.049\n",
       "rating_4.0        0.0184      0.014      1.341      0.184      -0.009       0.046\n",
       "rating_4.5        0.0197      0.013      1.542      0.127      -0.006       0.045\n",
       "rating_5.0        0.0104      0.016      0.651      0.517      -0.021       0.042\n",
       "type_Mexican     -0.0009      0.005     -0.177      0.860      -0.011       0.009\n",
       "type_Coffee      -0.0126      0.009     -1.443      0.153      -0.030       0.005\n",
       "type_Pizza        0.0050      0.008      0.625      0.534      -0.011       0.021\n",
       "type_Hotdogs     -0.0094      0.013     -0.735      0.464      -0.035       0.016\n",
       "type_Burgers     -0.0120      0.010     -1.238      0.219      -0.031       0.007\n",
       "type_Bakeries     0.0186      0.010      1.897      0.061      -0.001       0.038\n",
       "log_reviews       0.0002      0.001      0.262      0.794      -0.002       0.002\n",
       "==============================================================================\n",
       "Omnibus:                        5.425   Durbin-Watson:                   1.401\n",
       "Prob(Omnibus):                  0.066   Jarque-Bera (JB):                4.780\n",
       "Skew:                          -0.473   Prob(JB):                       0.0917\n",
       "Kurtosis:                       3.468   Cond. No.                     2.70e+03\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 2.7e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "const        5.471415e-104\n",
       "price_1.0     2.295518e-02\n",
       "dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.pvalues[model.pvalues<0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "const        13.577829\n",
       "price_1.0    -0.026399\n",
       "dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.params[model.params.index.isin(model.pvalues[model.pvalues<0.05].index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the R-squared is .62, meaning that our model explains 62% of the variance in home values. However, almost none of the coefficients are statistically significant. Only the coefficient on 1-dollar-sign restaurants is statistically significant with an $\\alpha$ of .05. The interpretation of the coefficient would be that a 1-standard-deviation increase in the number of 1-dollar-sign restaurants leads to a 2.6% decrease in average home values across all zip codes, on average and holding all other variables in our model constant. As such, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/Hovanes/dsi/Projects/project_4/project-client_project'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
